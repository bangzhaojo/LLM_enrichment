{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc0c133-403a-432a-a604-94f1654f43af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e9d837-f824-475b-b273-36df16e4d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the target api dataset\n",
    "with open('../../data/standard_semantic_features.json', 'r') as jsonfile:\n",
    "    format_venue_text = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6f8130-5b30-46f3-99e5-22f3216984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_with_true_values(d, prefix=''):\n",
    "    keys_with_true_values = []\n",
    "    \n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Recursively traverse nested dictionaries\n",
    "            keys_with_true_values.extend(get_keys_with_true_values(value, prefix + key + '.'))\n",
    "        elif value is True:\n",
    "            # If the value is True, add the key to the list\n",
    "            keys_with_true_values.append(prefix + key)\n",
    "    \n",
    "    return keys_with_true_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cafce17-370d-4666-a18f-76429676a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_venue_text = {}\n",
    "for venueid, info in format_venue_text.items():\n",
    "    prompt_str = \"\"\n",
    "    prompt_str += f\"Venue Name: {info['name']}.\\n\"\n",
    "    \n",
    "    if info['category']:\n",
    "        categories = ', '.join(info['category'])\n",
    "        prompt_str += f\"Venue Category: {categories}.\\n\"\n",
    "        \n",
    "    if info['description']:\n",
    "        prompt_str += f\" Venue Short Description: {info['description']}.\\n\"\n",
    "        \n",
    "    if info['price']:\n",
    "        if info['price'] == 1:\n",
    "            price = 'cheap'\n",
    "        elif info['price'] == 2:\n",
    "            price = 'moderate'\n",
    "        elif info['price'] == 3:\n",
    "            price = 'expensive'\n",
    "        else:\n",
    "            price = 'very expensive'\n",
    "            \n",
    "        prompt_str += f\" Venue price: {price}.\\n\"\n",
    "\n",
    "    if info['features']:\n",
    "        features = get_keys_with_true_values(info['features'], prefix='')\n",
    "        features = [f for f in features if 'payment' not in f]\n",
    "        features = [f.split('.')[-1] for f in features]\n",
    "        features = ', '.join(features)\n",
    "        \n",
    "        if len(features):\n",
    "            prompt_str += f\"The Features: {features}\\n\"\n",
    "            \n",
    "    if info['tips']:\n",
    "        tips = [f\" {i + 1}. {venue['text']}\" for i, venue in enumerate(info['tips'])]\n",
    "        tips_str = '\\n'.join(tips)\n",
    "        prompt_str += f\" The reviews of customers are:\\n{tips_str}.\"\n",
    "        \n",
    "    if info['rating']:\n",
    "        new_info = {}\n",
    "        new_info['instruction'] = \"\"\"Your task is to predict the rating of the venue based on its name, category, description, average price and customer reviews. The rating should be on a scale of 0.0 to 10.0, with precision limited to one decimal place.\"\"\"\n",
    "        new_info['input'] = prompt_str\n",
    "        \n",
    "        new_info['output'] = info['rating']\n",
    "        \n",
    "        prompt_venue_text[venueid] = new_info\n",
    "    else:\n",
    "        new_info = {}\n",
    "        new_info['instruction'] = \"\"\"Your task is to predict the rating of the venue based on its name, category, description, average price and customer reviews. The rating should be on a scale of 0.0 to 10.0, with precision limited to one decimal place.\"\"\"\n",
    "        new_info['input'] = prompt_str\n",
    "        new_info['output'] = None\n",
    "        prompt_venue_text[venueid] = new_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87e60273-531e-49cf-b43e-adb71ccd885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_venue_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c39f53e-0eed-4b9b-98c5-48eb381e0e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a4cadd3c134142af4013ac1d3ce858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "LORA_WEIGHTS = \"model/rating_lr1e-4_v2\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={'': 0},\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c4c80ff-9617-4068-81c8-2b837230bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n",
    "    \n",
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))\n",
    "\n",
    "def ask_alpaca(prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = generate_prompt(prompt)\n",
    "    # print(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    return format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebc007f1-5db2-4e56-be4a-625e80f38ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "1.4900908955446281 %\n",
      "2.9801817910892563 %\n",
      "4.470272686633884 %\n",
      "5.960363582178513 %\n",
      "7.450454477723141 %\n",
      "8.940545373267769 %\n",
      "10.430636268812398 %\n",
      "11.920727164357025 %\n",
      "13.410818059901656 %\n",
      "14.900908955446281 %\n",
      "16.390999850990912 %\n",
      "17.881090746535538 %\n",
      "19.371181642080167 %\n",
      "20.861272537624796 %\n",
      "22.351363433169425 %\n",
      "23.84145432871405 %\n",
      "25.33154522425868 %\n",
      "26.821636119803312 %\n",
      "28.311727015347937 %\n",
      "29.801817910892563 %\n",
      "31.291908806437192 %\n",
      "32.781999701981825 %\n",
      "34.27209059752645 %\n",
      "35.762181493071076 %\n",
      "37.2522723886157 %\n",
      "38.742363284160334 %\n",
      "40.232454179704966 %\n",
      "41.72254507524959 %\n",
      "43.21263597079422 %\n",
      "44.70272686633885 %\n",
      "46.192817761883475 %\n",
      "47.6829086574281 %\n",
      "49.172999552972726 %\n",
      "50.66309044851736 %\n",
      "52.153181344061984 %\n",
      "53.643272239606624 %\n",
      "55.13336313515125 %\n",
      "56.623454030695875 %\n",
      "58.1135449262405 %\n",
      "59.603635821785126 %\n",
      "61.09372671732976 %\n",
      "62.583817612874384 %\n",
      "64.07390850841901 %\n",
      "65.56399940396365 %\n",
      "67.05409029950827 %\n",
      "68.5441811950529 %\n",
      "70.03427209059753 %\n",
      "71.52436298614215 %\n",
      "73.01445388168678 %\n",
      "74.5045447772314 %\n",
      "75.99463567277603 %\n",
      "77.48472656832067 %\n",
      "78.97481746386529 %\n",
      "80.46490835940993 %\n",
      "81.95499925495456 %\n",
      "83.44509015049918 %\n",
      "84.93518104604381 %\n",
      "86.42527194158843 %\n",
      "87.91536283713306 %\n",
      "89.4054537326777 %\n",
      "90.89554462822232 %\n",
      "92.38563552376695 %\n",
      "93.87572641931158 %\n",
      "95.3658173148562 %\n",
      "96.85590821040083 %\n",
      "98.34599910594545 %\n",
      "99.83609000149009 %\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "predictions = {}\n",
    "for k, v in prompt_venue_text.items():\n",
    "    if v['output']:\n",
    "        predictions[k] = v\n",
    "    else:\n",
    "        v['output'] = ask_alpaca(v)\n",
    "        predictions[k] = v\n",
    "\n",
    "    if count % 100 == 0:\n",
    "        print(str(count/len(prompt_venue_text) * 100) + ' %')\n",
    "        filename = f\"../../result/rating_prediction_6711.json\"\n",
    "        with open(filename, \"w\") as json_file:\n",
    "            json.dump(predictions, json_file)\n",
    "    count += 1\n",
    "\n",
    "filename = f\"../../result/rating_prediction_6711.json\"\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fc91d-3b32-4a8b-bd53-43d66e6f1c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
