{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5060d8-ffa1-412a-8273-59448437b62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from transformers import GenerationConfig\n",
    "import textwrap\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c12d32-c3eb-4d20-9daa-c05aa532ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/keywords_without_label.json', 'r') as jsonfile:\n",
    "    prompt_venue_text = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf7f617-582c-4d5f-a2f1-a1d0648ca332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt_venue_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fd52bde-bd74-4320-8c88-27590ac5840e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ccf8cf46aa484e9857a81120a0ee94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = r\"model/llama2-7B-hf\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    #device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd02bbd7-f0a7-4fa3-b47e-a3df6e2a4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc52856-161b-44aa-ad5b-4aa8b48b0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE1 = \"\"\"\n",
    "Your task is to generate a series of keywords of the venue based on its name, category, description, average price, customer reviews, etc. The generated keywords should be able to describe every aspect of the venue well.\n",
    "\n",
    "### Example 1:\n",
    "\n",
    "#### Input:\n",
    "Venue Name: Chipotle Mexican Grill. \n",
    "Venue Category: Fast Food Restaurant, Mexican Restaurant. \n",
    "Venue Short Description: Chipotle is a restaurant that prepares burritos and tacos at reasonable prices. Our Food With Integrity philosophy includes unprocessed, sustainable, nutritious, responsibly raised, and organic foods where possible.. \n",
    "Venue price: cheap. \n",
    "The Open Hour: Mon-Fri 10:00 AM-10:00 PM; Sat-Sun 10:45 AM-10:00 PM \n",
    "The Features: cocktails, brunch, lunch, happy_hour, dinner, music, live_music\n",
    "The Customer Reviews: \n",
    " 1. If you ask for double meat before the rice, sometimes they’ll forget to charge you extra. \n",
    " 2. Disrespectful staff. I asked for extra chicken and rice on my burrito. The rice made it in but the chicken was thrown in the bag with no lid. And they rolled their eyes when I asked! Never again! \n",
    " 3. Good portions. \n",
    " 4. They're pretty speedy despite the insane lunch line!! 5. Great place for a quick chicken steak or pork bowl.\n",
    "\n",
    "#### Response:\n",
    "meats, good service, chicken, well, lunch, cocktails, good for working, rice, lines, dinner, great value, burritos, onions, margaritas, dips, salsa, trendy, corn, good for a quick meal, barbacoa, black beans, red chili.\n",
    "\n",
    "### Example 2:\n",
    "\n",
    "#### Input:\n",
    "Venue Name: Tony's Pizza. \n",
    "Venue Category: Pizzeria. \n",
    "Venue price: cheap. \n",
    "The Open Hour: Mon-Thu 10:00 AM-11:00 PM; Fri-Sat 10:00 AM-11:59 PM; Sun 10:30 AM-11:00 PM \n",
    "The Features: brunch, lunch, dinner, delivery \n",
    "The Customer Reviews: \n",
    " 1. What you order isn’t necessarily what you’ll get. Unless perhaps you order cramps and indigestion. \n",
    " 2. Love this spot :) cheap and good pizza \n",
    " 3. Simpler than Carmine's but just as good if not better. \n",
    " 4. Eat your upside down slice fuckin backwards \n",
    " 5. If you get pizza on Graham Ave, you're either a Tony's guy or a Carmine's guy. I'm a Carmine's guy..\n",
    "\n",
    "#### Response:\n",
    "chicken, pizza, soup, cheese, city, great value, bacon, good for a quick meal, hot sauce, Sicilian, garlic knots, carmine.\n",
    "\n",
    "### Example 3:\n",
    "\n",
    "#### Input:\n",
    "Venue Name: Panera Bread. \n",
    "Venue Category: Bakery, Cafe, Coffee, and Tea House, Fast Food Restaurant. \n",
    "Venue Short Description: From focusing on quality, clean ingredients to serving our food to you in a warm and welcoming environment, Panera Bread is committed to being an ally to our guests. That means crafting a menu of soups, salads and sandwiches that we are proud to feed our families.... \n",
    "Venue price: moderate. \n",
    "The Open Hour: Mon-Sat 8:00 AM-8:00 PM \n",
    "The Features: full_bar, delivery \n",
    "The Customer Reviews: \n",
    " 1. Easy off and on the highway \n",
    " 2. I love the savory flavor of the French onion soup. My new fave is the cinnamon crunch bagel toasted with cream cheese \n",
    " 3. Smoked chicken. Fresh avocado. Melted smoked Gouda. And freshly-baked Black Pepper Focaccia. Just a few good reasons to crave our Chipotle Chicken Avocado Melt. At Panera. Food as it should be. \n",
    "\n",
    "#### Response:\n",
    "restaurants, bar, alcohol, good service, breakfast food, sandwiches, soup, lunch, good for working, cake, bread, dinner, chili, trendy, good for a quick meal, cream cheese, iced green tea.\n",
    "\n",
    "### Your Task:\n",
    "#### Input:\n",
    "{\"[INPUT]\"}\n",
    "\n",
    "#### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1149b7-09c8-49f0-8cb0-3cda5d28e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(text_input: str, model):\n",
    "    \n",
    "    prompt = PROMPT_TEMPLATE1.replace(\"[INPUT]\", text_input).replace(\"'\", '')\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        response = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n",
    "    \n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"#### Response:\")[4].strip()\n",
    "    output = \"\\n\".join(textwrap.wrap(response))\n",
    "    return output\n",
    "\n",
    "def load_checkpoint(filepath=None):\n",
    "    \n",
    "    if filepath == None:\n",
    "        return {}\n",
    "    \n",
    "    # Provide the path to your JSON file\n",
    "    text_file = filepath\n",
    "\n",
    "    # Load JSON data from the file\n",
    "    with open(text_file, \"r\") as json_file:\n",
    "        generated_formatted_venue_text = json.load(json_file)\n",
    "        \n",
    "    return generated_formatted_venue_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21932b70-ca9f-4808-ae08-a2ad82dd41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = []\n",
    "labels = []\n",
    "\n",
    "for idx, venue in prompt_venue_text.items():\n",
    "    text_inputs.append(venue['text'])\n",
    "    labels.append(venue['truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ec91e2-727b-4c18-9be9-15dae1e258ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Venue Name: Pearl Art & Craft Supply.\\nVenue Category: Arts and Crafts Store.\\nThe Customer Reviews:\\n 1. A paradise for any kind of artist, or even if you're not one yet. 10% discount if you have a student or teacher ID !\\n 2. Take the stairs\\n 3. Great place to buy all you need, but the place looks like its going to collapse\\n 4. Muy buena selección de cosas y personal experto, un poco en caida pero aun asi de lo mejor de ny\\n 5. There's an elevator in the back!.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae822ae8-d98f-498d-8672-dff1ed89b612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def generate_batch_responses(text_inputs, model, tokenizer, device, batch_size=8):\n",
    "    responses = []\n",
    "    \n",
    "    # Prepare batches of prompts\n",
    "    for i in range(0, len(text_inputs), batch_size):\n",
    "        batch_prompts = text_inputs[i:i + batch_size]\n",
    "        prompts = [PROMPT_TEMPLATE1.replace(\"[INPUT]\", text).replace(\"'\", '') for text in batch_prompts]\n",
    "        \n",
    "        # Tokenize all prompts in the current batch\n",
    "        #encoding = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "        encoding = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].to(device)\n",
    "        \n",
    "        # Generate responses for the entire batch\n",
    "        with torch.inference_mode():\n",
    "            batch_responses = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=128,  # Assuming 128 is the desired maximum length\n",
    "                temperature=0.1,\n",
    "                top_p=0.75,\n",
    "                repetition_penalty=1.1,\n",
    "                #pad_token_id=tokenizer.eos_token_id,  # Ensure padding with the EOS token\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "            )\n",
    "        \n",
    "        # Decode each response in the batch\n",
    "        for response in batch_responses.sequences:\n",
    "            decoded_output = tokenizer.decode(response)\n",
    "            split_response = decoded_output.split(\"#### Response:\")[4].strip()\n",
    "            wrapped_response = \"\\n\".join(textwrap.wrap(split_response))\n",
    "            responses.append(wrapped_response)\n",
    "            \n",
    "        if i % 10 == 0:\n",
    "            filename = \"data/keywords/keywords_predict_name_des_tip_batch.json\"\n",
    "            with open(filename, \"w\") as json_file:\n",
    "                json.dump(responses, json_file)\n",
    "\n",
    "    return responses\n",
    "\n",
    "generated_formatted_venue_text = generate_batch_responses(\n",
    "    text_inputs=text_inputs,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    "    batch_size=8  # Adjust batch size as needed based on available memory\n",
    ")\n",
    "\n",
    "# Save the generated responses to a file\n",
    "filename = \"../../result/keywords_predict_name_des_tip_batch.json\"\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(generated_formatted_venue_text, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca517bbd-c0a3-40dc-8c43-1ee5bb91ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
