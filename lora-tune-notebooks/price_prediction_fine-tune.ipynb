{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aed4006-87f8-488f-be4d-b0e9783bbec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795a865-73fc-45b2-abf6-e9901441ee02",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c4ccde-fdf0-4a18-bc91-ab33268e607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"model/llama2-7B-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830bd52d-9f70-4b9f-bb75-7294f601f524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b692053db04e9bb1e9515b8157768d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e74c16b-2ee4-4706-8c7d-5a841c0d8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # continue fine-tuning\n",
    "# BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "# LORA_WEIGHTS = \"model/llama2_epoch15_lr4e-3/checkpoint-148\"\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL,\n",
    "#     load_in_8bit=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={'': 0},\n",
    "#     local_files_only=True,\n",
    "# )\n",
    "\n",
    "# model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a5e405-5094-4217-a86e-6ceec9535e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd18e503-a1fd-4640-9b6f-ece542234a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/bangzhao/.cache/huggingface/datasets/json/default-c8c012706a999bea/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8705ac186824a3ea792e12ade670c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 3156\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"../data/alpaca-api-enrichment-price-dataset.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7696c80e-1754-496f-95b2-2218acf316fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Your task is to predict the average consuming price of a venue based on its description, which includes its name and category. The venue's price will fall into one of four categories: Cheap, Moderate, Expensive, and Very Expensive. Remember, the name and category of the venue can be significant indicators of its price. For instance, fast-food chains like 'McDonald' might typically be 'Cheap', while upscale restaurants with names suggesting fine dining might be 'Expensive' or 'Very Expensive.'\",\n",
       " 'input': \"Venue Name: Taïm Mobile Falafel & Smoothie Truck.\\nVenue Category: Food Truck.\\n Venue Short Description: Taim's famous falafel and fresh fruit smoothies roaming the streets of NYC on wheels! Vegan and gluten free options, and don't forget, we're kosher..\\nThe Features: lunch, takeout\\n The reviews of customers are:\\n 1. Taïm\\xa0offers a high-end twist on Tel Aviv street food. The falafel sandwich—green falafel, hummus, tahini and Israeli salad, all nestled inside a thick, warm blanket of pita—is seriously life-changing.\\n 2. the falafel sandwich is the best. the falafel are like pudding. other placeCouldn't get a sabich sandwich, so was disappointed about that.\\n 3. Falafel platter\\n 4. Not there any more? Called the phone number and the person on the other end said he didn't know anything about a food truck...\\n 5. The dates in the banana/date/lime smoothie get caught in the straw. It's great nonetheless..\",\n",
       " 'output': 'Cheap'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c617ca6-4a4b-4a68-a2bb-e49866a13fca",
   "metadata": {},
   "source": [
    "### generate prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a0364fc-87fa-43e3-ab5a-2d0575357653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6fe7f2-d7f6-499e-9478-408890f63dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500fe01d-ce8e-466a-b531-7b9938e95f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/bangzhao/.cache/huggingface/datasets/json/default-c8c012706a999bea/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0cc6d9c8621ec7ba.arrow and /home/bangzhao/.cache/huggingface/datasets/json/default-c8c012706a999bea/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-92003ed07fa4138c.arrow\n",
      "Loading cached processed dataset at /home/bangzhao/.cache/huggingface/datasets/json/default-c8c012706a999bea/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f2773ef1429e4811.arrow\n",
      "Loading cached processed dataset at /home/bangzhao/.cache/huggingface/datasets/json/default-c8c012706a999bea/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f5be150cca57f1a1.arrow\n"
     ]
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=0.25, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93a2185-12b2-41f5-b3ae-843e1cb509ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 2367\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 789\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d09d0c-a7ef-4be8-b3f9-0ebe51a67c81",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b8ba42-c4e0-445e-80c2-d732cf19059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8 # attention head\n",
    "LORA_ALPHA = 16 # alpha scaling\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "OUTPUT_DIR = \"model/price_lr1e-4_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bf5af5e-3348-477a-b722-fadfec8de7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e003796-1a29-49d1-80cb-dd7960f115f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps = int(len(train_val['train']) / MICRO_BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c63dc35f-4da9-4f86-9a53-5819bc960002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d14ab2ba-46a6-450b-aa2f-d466fe2837fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1184' max='1184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1184/1184 1:58:22, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.746346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.719400</td>\n",
       "      <td>0.745417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.720600</td>\n",
       "      <td>0.744675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71312b88-3abb-4a18-9114-6afe1c605b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()\n",
    " \n",
    "model.push_to_hub(\"bangzhao/llama2_lora_api_enrichment_price_predictor\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f84fc-05b2-43a2-8fa1-4907bcfe3acc",
   "metadata": {},
   "source": [
    "### Load Model and lora weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965a50fb-578a-45ca-a198-047dd1fdd8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa62a39257754596a62cf2dfe779c51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "LORA_WEIGHTS = \"model/price_lr7e-5\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={'': 0},\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44ccfc3e-1de9-4dcf-9c38-24a9b9d9e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n",
    "    \n",
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))\n",
    "\n",
    "def ask_alpaca(prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = generate_prompt(prompt)\n",
    "    #print(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    return format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7b95016-827e-4483-802a-97c71ce77801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Moderate</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_alpaca(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb4bd1c-d6d2-47f4-8fb6-77f0e36c0d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 789\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e95a0f25-f14f-4f0a-b432-e239f8eac9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moderate', 'Cheap', 'Cheap', 'Expensive']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['output'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ea811e9-e6b7-4214-9c53-4608c926def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "6.34 %\n",
      "12.67 %\n",
      "19.01 %\n",
      "25.35 %\n",
      "31.69 %\n",
      "38.02 %\n",
      "44.36 %\n",
      "50.7 %\n",
      "57.03 %\n",
      "63.37 %\n",
      "69.71 %\n",
      "76.05 %\n",
      "82.38 %\n",
      "88.72 %\n",
      "95.06 %\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for idx, venue in enumerate(val_data):\n",
    "    \n",
    "    \n",
    "    prediction = {}\n",
    "    prediction['text'] = venue['input']\n",
    "    prediction['truth'] = venue['output']\n",
    "    prediction['generated'] = ask_alpaca(venue)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(str(round(idx/len(val_data) * 100, 2)) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919a63a-e78e-4cc0-b27a-1e0ef880c81f",
   "metadata": {},
   "source": [
    "### make predicted text numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c194241-09f5-449b-8abf-05345105d542",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2861452/4233535466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvenue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'cheap'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mvenue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'moderate'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvenue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvenue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "for venue in predictions:\n",
    "    if 'cheap' in venue['truth'].lower():\n",
    "        venue['truth'] = 1\n",
    "    elif 'moderate' in venue['truth'].lower():\n",
    "        venue['truth'] = 2 \n",
    "    elif 'expensive' in venue['truth'].lower() and ('very' not in venue['truth'].lower()):\n",
    "        venue['truth'] = 3\n",
    "    elif 'very expensive' in venue['truth'].lower():\n",
    "        venue['truth'] = 4\n",
    "\n",
    "    else:\n",
    "        venue['truth'] = None\n",
    "        \n",
    "    if 'cheap' in venue['generated'].lower():\n",
    "        venue['generated'] = 1\n",
    "    elif 'moderate' in venue['generated'].lower():\n",
    "        venue['generated'] = 2 \n",
    "    elif 'expensive' in venue['generated'].lower() and ('very' not in venue['generated'].lower()):\n",
    "        venue['generated'] = 3\n",
    "    elif 'very expensive' in venue['generated'].lower():\n",
    "        venue['generated'] = 4\n",
    "    else:\n",
    "        venue['generated'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3927060a-9f7e-4180-b514-d8488c53de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = f\"../result/price_predict_name_des_cate_v2_epoch8.json\"\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b40ac355-d5ef-4e44-95ea-ffcd05bd8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0edddca9-a7d4-4cfa-8b70-f60b493df41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>truth</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Venue Name: Taproom No. 307.\\n Venue Category:...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Venue Name: Food Gallery 32.\\n Venue Category:...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Venue Name: Bagels on the Square.\\n Venue Cate...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Venue Name: The Sunburnt Calf.\\n Venue Categor...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Venue Name: Ost Cafe.\\n Venue Category: Coffee...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>Venue Name: Morning Coffee.\\n Venue Category: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Venue Name: Nespresso Boutique Bar, SoHo.\\n Ve...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Venue Name: Dunkin'.\\n Venue Category: Coffee ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>Venue Name: Tropix Bar &amp; Lounge.\\n Venue Categ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Venue Name: Littleneck.\\n Venue Category: Rest...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>789 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  truth  generated\n",
       "0    Venue Name: Taproom No. 307.\\n Venue Category:...      2          2\n",
       "1    Venue Name: Food Gallery 32.\\n Venue Category:...      1          2\n",
       "2    Venue Name: Bagels on the Square.\\n Venue Cate...      1          1\n",
       "3    Venue Name: The Sunburnt Calf.\\n Venue Categor...      3          2\n",
       "4    Venue Name: Ost Cafe.\\n Venue Category: Coffee...      1          2\n",
       "..                                                 ...    ...        ...\n",
       "784  Venue Name: Morning Coffee.\\n Venue Category: ...      1          2\n",
       "785  Venue Name: Nespresso Boutique Bar, SoHo.\\n Ve...      2          2\n",
       "786  Venue Name: Dunkin'.\\n Venue Category: Coffee ...      1          1\n",
       "787  Venue Name: Tropix Bar & Lounge.\\n Venue Categ...      2          2\n",
       "788  Venue Name: Littleneck.\\n Venue Category: Rest...      2          2\n",
       "\n",
       "[789 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbf4d6-b848-4d3e-813b-90c255678215",
   "metadata": {},
   "source": [
    "### some basic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "873ecfd0-7d02-4a32-b21b-957075100ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8ce6d8d-0628-4ff4-8df6-a337a31489d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.44740177439797213\n",
      "Mean Squared Error (MSE): 0.467680608365019\n",
      "Root Mean Squared Error (RMSE): 0.6838717777222708\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(predictions_df['truth'], predictions_df['generated'])\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(predictions_df['truth'], predictions_df['generated'])\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac34eb33-7a9d-4754-876d-c05e1c6e677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.5361216730038023\n",
      "Mean Squared Error (MSE): 0.5640050697084917\n",
      "Root Mean Squared Error (RMSE): 0.7510027095214049\n"
     ]
    }
   ],
   "source": [
    "dummy1 = [2] * len(predictions)\n",
    "mae = mean_absolute_error(predictions_df['truth'],dummy1)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "mse = mean_squared_error(predictions_df['truth'], dummy1)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ea67144-d6c8-4f80-a6cf-3d5130c26961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.2877059569074778\n",
      "Mean Squared Error (MSE): 2.0836501901140685\n",
      "Root Mean Squared Error (RMSE): 1.4434854312094973\n"
     ]
    }
   ],
   "source": [
    "dummy2 = [3] * len(predictions)\n",
    "\n",
    "mae = mean_absolute_error(predictions_df['truth'],dummy2)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "mse = mean_squared_error(predictions_df['truth'], dummy2)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60319fb5-876d-46b2-8fcf-c4459501ca78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
