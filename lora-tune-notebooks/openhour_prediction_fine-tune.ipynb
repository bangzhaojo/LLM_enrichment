{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ef6f61-18e1-4756-9780-3831b52d18bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "#DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e485ee6-29e1-4748-9efb-a8559c1d1dbd",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ca38cd-8e6a-4428-a278-5f76753beec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"model/llama2-7B-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50657035-d555-47e3-897b-92da6008dce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ff18e1598042bbbbbcce7e6b1371b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccda70b0-96b8-467e-852f-e97ca480258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # continue fine-tuning\n",
    "# BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "# LORA_WEIGHTS = \"model/llama2_epoch12_lr3e-5_openhour/checkpoint-2052\"\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     BASE_MODEL,\n",
    "#     load_in_8bit=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={'': 0},\n",
    "#     local_files_only=True,\n",
    "# )\n",
    "\n",
    "# model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee19b010-b447-473b-b2d8-be179492912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc89d0b-845a-4c47-a398-e16f9c8902f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/bangzhao/.cache/huggingface/datasets/json/default-3c96d78c638832ba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ef7679aca84de6b8b5081c54203a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 3632\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"../data/alpaca-api-enrichment-openhour-dataset.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c064bc0-82cb-4cc2-a531-856f675f8197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Your task is to predict the open hour of the venue based on its name, category, description, average price, customer reviews and other features. ',\n",
       " 'input': 'Venue Name: Verrazzano-Narrows Bridge.\\nVenue Category: Bridge.\\nVenue Short Description: The Verrazzano-Narrows Bridge is a double-decked suspension bridge in the U.S. state of New York that connects the New York City boroughs of Staten Island and Brooklyn..\\nThe Customer Reviews:\\n 1. Avoid the Verrazano Bridge during storms and heavy gusts of wind.\\n 2. named for the italian explorer giovanni da verrazzano, though for about half a century it was officially spelled “verrazano”\\n 3. Best views ever!\\n 4. Bottom level usually less traffic but beautiful views.\\n 5. Beautiful streamlined bridge. Love seeing it from a distance. Watched it being built and drove over it the day it opened to the public..',\n",
       " 'output': 'Open Daily 12:00 AM-12:00 AM'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c0fd6-cc4a-4044-b556-5df66120f841",
   "metadata": {},
   "source": [
    "### Generate Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968457fe-8869-4acb-aa62-729473798314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129d6b6f-3f80-4eab-905b-f9848f5c03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8cd8579-31b3-495e-b7e6-00f8f2555b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/bangzhao/.cache/huggingface/datasets/json/default-3c96d78c638832ba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9fd891e5cfca29b9.arrow and /home/bangzhao/.cache/huggingface/datasets/json/default-3c96d78c638832ba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-691a4ece1590b43b.arrow\n",
      "Loading cached processed dataset at /home/bangzhao/.cache/huggingface/datasets/json/default-3c96d78c638832ba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-da59bc0f4127ee52.arrow\n",
      "Loading cached processed dataset at /home/bangzhao/.cache/huggingface/datasets/json/default-3c96d78c638832ba/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e4f2bdb224b306d2.arrow\n"
     ]
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=0.25, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1f81eb-9e40-4783-8d93-e5067a2e685f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 2724\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 908\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d0c4d-381a-4eb7-9543-36434875aa7b",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a515a5-398b-4a32-a04d-16409b8b643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_R = 8 # attention head\n",
    "LORA_ALPHA = 16 # alpha scaling\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "MICRO_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 7e-5\n",
    "OUTPUT_DIR = \"model/open_hour_lr7e-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f0ff00-b81d-4516-8c73-9ee4efcf786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fbdfb1f-469f-4d2c-b193-749bdc700cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=MICRO_BATCH_SIZE,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps = int(len(train_val['train']) / MICRO_BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9454913a-32ce-49f3-9e30-b87118cee0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "634b1026-699b-4fb5-b1eb-5277806725fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1710/1710 6:57:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.258600</td>\n",
       "      <td>1.004652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.976500</td>\n",
       "      <td>0.951041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>0.917820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.897600</td>\n",
       "      <td>0.902191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>0.892858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.873200</td>\n",
       "      <td>0.885319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.880928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.878467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.876848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>0.876355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/bangzhao/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (\n",
    "#     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "#         self, old_state_dict()\n",
    "#     )\n",
    "# ).__get__(model, type(model))\n",
    "\n",
    "model = torch.compile(model)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f7f59c-94ac-4582-88df-b143fbd85c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792cdd62fc9e4e788ee403c1536243e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d57fc0e4144bfbb748a564a3d18940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/bangzhao/llama2_lora_api_enrichment_open_hour_predictor/commit/5b01996113c9a1227348bd900e9608adc278b3e7', commit_message='Upload model', commit_description='', oid='5b01996113c9a1227348bd900e9608adc278b3e7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()\n",
    " \n",
    "model.push_to_hub(\"bangzhao/llama2_lora_api_enrichment_open_hour_predictor\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc420f5-1c48-4e7f-b05d-94daec875ed6",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9484fad-2ad4-43bd-94c9-28c28a097edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfe8f3e0ca640d0919785350d90f0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_MODEL = \"model/llama2-7B-hf\"\n",
    "LORA_WEIGHTS = \"model/open_hour_lr7e-5\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={'': 0},\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb89b485-3e2b-403a-ad6a-f4a0b5d59073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=128,\n",
    "        )\n",
    "    \n",
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "    return \"\\n\".join(textwrap.wrap(response))\n",
    "\n",
    "def ask_alpaca(prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = generate_prompt(prompt)\n",
    "    #print(prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    return format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee02312a-628e-4ba1-b6f6-c3a2c4ea34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c909254b-758f-42ce-a15e-eef4c991211f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mon-Thu 11:30 AM-9:00 PM; Fri-Sun 11:30 AM-10:00 PM</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_alpaca(val_data[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd7238-dd08-409f-b526-7d37d55f4888",
   "metadata": {},
   "source": [
    "### Load model and Lora weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e9f1030-01b1-4f49-8e2b-efbf8219b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "0.55 %\n",
      "1.1 %\n",
      "1.65 %\n",
      "2.2 %\n",
      "2.75 %\n",
      "3.3 %\n",
      "3.85 %\n",
      "4.41 %\n",
      "4.96 %\n",
      "5.51 %\n",
      "6.06 %\n",
      "6.61 %\n",
      "7.16 %\n",
      "7.71 %\n",
      "8.26 %\n",
      "8.81 %\n",
      "9.36 %\n",
      "9.91 %\n",
      "10.46 %\n",
      "11.01 %\n",
      "11.56 %\n",
      "12.11 %\n",
      "12.67 %\n",
      "13.22 %\n",
      "13.77 %\n",
      "14.32 %\n",
      "14.87 %\n",
      "15.42 %\n",
      "15.97 %\n",
      "16.52 %\n",
      "17.07 %\n",
      "17.62 %\n",
      "18.17 %\n",
      "18.72 %\n",
      "19.27 %\n",
      "19.82 %\n",
      "20.37 %\n",
      "20.93 %\n",
      "21.48 %\n",
      "22.03 %\n",
      "22.58 %\n",
      "23.13 %\n",
      "23.68 %\n",
      "24.23 %\n",
      "24.78 %\n",
      "25.33 %\n",
      "25.88 %\n",
      "26.43 %\n",
      "26.98 %\n",
      "27.53 %\n",
      "28.08 %\n",
      "28.63 %\n",
      "29.19 %\n",
      "29.74 %\n",
      "30.29 %\n",
      "30.84 %\n",
      "31.39 %\n",
      "31.94 %\n",
      "32.49 %\n",
      "33.04 %\n",
      "33.59 %\n",
      "34.14 %\n",
      "34.69 %\n",
      "35.24 %\n",
      "35.79 %\n",
      "36.34 %\n",
      "36.89 %\n",
      "37.44 %\n",
      "38.0 %\n",
      "38.55 %\n",
      "39.1 %\n",
      "39.65 %\n",
      "40.2 %\n",
      "40.75 %\n",
      "41.3 %\n",
      "41.85 %\n",
      "42.4 %\n",
      "42.95 %\n",
      "43.5 %\n",
      "44.05 %\n",
      "44.6 %\n",
      "45.15 %\n",
      "45.7 %\n",
      "46.26 %\n",
      "46.81 %\n",
      "47.36 %\n",
      "47.91 %\n",
      "48.46 %\n",
      "49.01 %\n",
      "49.56 %\n",
      "50.11 %\n",
      "50.66 %\n",
      "51.21 %\n",
      "51.76 %\n",
      "52.31 %\n",
      "52.86 %\n",
      "53.41 %\n",
      "53.96 %\n",
      "54.52 %\n",
      "55.07 %\n",
      "55.62 %\n",
      "56.17 %\n",
      "56.72 %\n",
      "57.27 %\n",
      "57.82 %\n",
      "58.37 %\n",
      "58.92 %\n",
      "59.47 %\n",
      "60.02 %\n",
      "60.57 %\n",
      "61.12 %\n",
      "61.67 %\n",
      "62.22 %\n",
      "62.78 %\n",
      "63.33 %\n",
      "63.88 %\n",
      "64.43 %\n",
      "64.98 %\n",
      "65.53 %\n",
      "66.08 %\n",
      "66.63 %\n",
      "67.18 %\n",
      "67.73 %\n",
      "68.28 %\n",
      "68.83 %\n",
      "69.38 %\n",
      "69.93 %\n",
      "70.48 %\n",
      "71.04 %\n",
      "71.59 %\n",
      "72.14 %\n",
      "72.69 %\n",
      "73.24 %\n",
      "73.79 %\n",
      "74.34 %\n",
      "74.89 %\n",
      "75.44 %\n",
      "75.99 %\n",
      "76.54 %\n",
      "77.09 %\n",
      "77.64 %\n",
      "78.19 %\n",
      "78.74 %\n",
      "79.3 %\n",
      "79.85 %\n",
      "80.4 %\n",
      "80.95 %\n",
      "81.5 %\n",
      "82.05 %\n",
      "82.6 %\n",
      "83.15 %\n",
      "83.7 %\n",
      "84.25 %\n",
      "84.8 %\n",
      "85.35 %\n",
      "85.9 %\n",
      "86.45 %\n",
      "87.0 %\n",
      "87.56 %\n",
      "88.11 %\n",
      "88.66 %\n",
      "89.21 %\n",
      "89.76 %\n",
      "90.31 %\n",
      "90.86 %\n",
      "91.41 %\n",
      "91.96 %\n",
      "92.51 %\n",
      "93.06 %\n",
      "93.61 %\n",
      "94.16 %\n",
      "94.71 %\n",
      "95.26 %\n",
      "95.81 %\n",
      "96.37 %\n",
      "96.92 %\n",
      "97.47 %\n",
      "98.02 %\n",
      "98.57 %\n",
      "99.12 %\n",
      "99.67 %\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for idx, venue in enumerate(val_data):\n",
    "    \n",
    "    \n",
    "    prediction = {}\n",
    "    prediction['text'] = venue['input']\n",
    "    prediction['truth'] = venue['output']\n",
    "    prediction['generated'] = ask_alpaca(venue)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    if idx % 5 == 0:\n",
    "        print(str(round(idx/len(val_data) * 100, 2)) + \" %\")\n",
    "        filename = f\"../result/openhour_predict_name_des_cate_pri_fea_tip_epoch10.json\"\n",
    "        with open(filename, \"w\") as json_file:\n",
    "            json.dump(predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73384541-1007-46eb-9adc-a657f4f6d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"../result/openhour_predict_name_des_cate_pri_fea_tip_epoch10.json\"\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(predictions, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea2d21-13c9-4fa3-9468-ffbcbc5fd3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c29b9e-3785-4174-b29a-f5fc1ca01bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
